{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"hospital-review-ai-train.csv\", \"test\": \"hospital-review-ai-test.csv\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 61/61 [00:00<00:00, 1226.60 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preprocess data and tokenize text\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_dataset = dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=True), batched=True)\n",
    "\n",
    "# Define model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)  # Assuming 6 sentiment classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Define training arguments and instantiate Trainer object\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Specify the output directory\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"test\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 39/39 [14:39<00:00, 22.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 879.0026, 'train_samples_per_second': 0.334, 'train_steps_per_second': 0.044, 'train_loss': 1.438057630490034, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=1.438057630490034, metrics={'train_runtime': 879.0026, 'train_samples_per_second': 0.334, 'train_steps_per_second': 0.044, 'train_loss': 1.438057630490034, 'epoch': 3.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the trained model\n",
    "model.save_pretrained(\"./sentiment_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage for sentiment prediction\n",
    "model = model.from_pretrained(\"./sentiment_model\")\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def predict_sentiment(review_text, max_length=512):  # Adjust max_length according to your model's maximum input length\n",
    "    inputs = tokenizer(review_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = torch.argmax(outputs.logits[0]).item()\n",
    "    return predicted_class\n",
    "\n",
    "# Example usage for sentiment prediction\n",
    "review_text = \"The hospital staff was very friendly and helpful!\"\n",
    "predicted_sentiment = predict_sentiment(review_text)\n",
    "print(\"Predicted Sentiment:\", predicted_sentiment)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Class 0: Probability 0.0919962078332901\n",
      "Sentiment Class 1: Probability 0.06554585695266724\n",
      "Sentiment Class 2: Probability 0.09690117090940475\n",
      "Sentiment Class 3: Probability 0.10259004682302475\n",
      "Sentiment Class 4: Probability 0.2855378985404968\n",
      "Sentiment Class 5: Probability 0.35742881894111633\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def predict_sentiment(review_text, tokenizer, model):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(review_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Perform forward pass\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Apply softmax to convert logits to probabilities\n",
    "    probs = F.softmax(outputs.logits, dim=1)\n",
    "    \n",
    "    # Convert tensor to list for easier manipulation\n",
    "    probs = probs.squeeze().tolist()\n",
    "    \n",
    "    # Get all possible sentiments along with their probabilities\n",
    "    sentiment_probabilities = [(class_idx, prob) for class_idx, prob in enumerate(probs)]\n",
    "    \n",
    "    return sentiment_probabilities\n",
    "\n",
    "# Example usage\n",
    "review_text = \"The hospital staff was very friendly and helpful!!\"\n",
    "sentiment_probabilities = predict_sentiment(review_text, tokenizer, model)\n",
    "\n",
    "# Output the sentiment probabilities\n",
    "for class_idx, probability in sentiment_probabilities:\n",
    "    print(f\"Sentiment Class {class_idx}: Probability {probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3770491803278688\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = load_dataset(\"csv\", data_files={\"test\": \"hospital-review-ai-test.csv\"})\n",
    "\n",
    "# Tokenize test dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "encoded_test_dataset = test_dataset.map(lambda examples: tokenizer(examples[\"text\"], truncation=True, padding=True), batched=True)\n",
    "\n",
    "# Load trained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./sentiment_model\")\n",
    "model.eval()\n",
    "\n",
    "# Run inference on test dataset\n",
    "predictions = []\n",
    "for example in encoded_test_dataset[\"test\"]:\n",
    "    inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    predicted_class = outputs.logits.argmax().item()\n",
    "    predictions.append(predicted_class)\n",
    "\n",
    "# Evaluate predictions\n",
    "true_labels = test_dataset[\"test\"][\"label\"]\n",
    "accuracy = sum(pred == label for pred, label in zip(predictions, true_labels)) / len(true_labels)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
